{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this Notebook:\n",
    "- Load images into the `data/images` dir, with unique names and `.png` endings\n",
    "- Load metadata into the `data/metadata`, with the name `images.json`\n",
    "\n",
    "You can use this notebook to load the sample dataset, or load any other dataset you have, it must just fulfill the specifications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/venv/lib/python3.11/site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging in /opt/venv/lib/python3.11/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/venv/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/venv/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/venv/lib/python3.11/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/venv/lib/python3.11/site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/venv/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/venv/lib/python3.11/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.11/site-packages (from requests->kagglehub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install kaggle if a sample dataset should be loaded, otherwise add your own dataset directly\n",
    "%pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /root/.cache/kagglehub/datasets/vishalsubbiah/pokemon-images-and-types/versions/4\n",
      "\n",
      "=== Paths ===\n",
      "Current working directory: /app/notebooks\n",
      "Repository root: /app\n",
      "Dataset path: /app/data\n",
      "Image directory: /app/data/images\n",
      "Metadata path: /app/data/metadata/images.json\n",
      "\n",
      "=== Loading Data ===\n",
      "Loading CSV from: /root/.cache/kagglehub/datasets/vishalsubbiah/pokemon-images-and-types/versions/4/pokemon.csv\n",
      "Loading images from: /root/.cache/kagglehub/datasets/vishalsubbiah/pokemon-images-and-types/versions/4/images\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# --- Path Definitions ---\n",
    "# Base directories as per your Docker mount configuration\n",
    "NOTEBOOK_DIR = Path(os.getcwd())\n",
    "REPO_ROOT = NOTEBOOK_DIR.parent\n",
    "DATASET_PATH = REPO_ROOT / \"data\"\n",
    "\n",
    "# Image and metadata directories\n",
    "IMAGE_DIR = DATASET_PATH / \"images\"\n",
    "METADATA_PATH = DATASET_PATH / \"metadata\" / \"images.json\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Download Dataset ---\n",
    "# Download dataset using kagglehub\n",
    "kaggle_path = kagglehub.dataset_download(\"vishalsubbiah/pokemon-images-and-types\")\n",
    "print(\"Dataset downloaded to:\", kaggle_path)\n",
    "\n",
    "# --- Kaggle Paths ---\n",
    "KAGGLE_IMAGES = Path(kaggle_path) / \"images\"\n",
    "KAGGLE_CSV = Path(kaggle_path) / \"pokemon.csv\"\n",
    "\n",
    "# Also define them as strings (if needed for other libraries)\n",
    "KAGGLE_IMAGES_STR = os.path.join(kaggle_path, \"images\")\n",
    "KAGGLE_CSV_STR = os.path.join(kaggle_path, \"pokemon.csv\")\n",
    "\n",
    "# --- Log Paths ---\n",
    "print(\"\\n=== Paths ===\")\n",
    "print(f\"Current working directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Repository root: {REPO_ROOT}\")\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Image directory: {IMAGE_DIR}\")\n",
    "print(f\"Metadata path: {METADATA_PATH}\")\n",
    "\n",
    "print(\"\\n=== Loading Data ===\")\n",
    "print(f\"Loading CSV from: {KAGGLE_CSV_STR}\")\n",
    "print(f\"Loading images from: {KAGGLE_IMAGES_STR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 809 Pokemon from CSV\n",
      "Saved metadata to: /app/data/metadata/images.json\n",
      "Processed 809 images to: /app/data/images\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Process Data ---\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "\n",
    "def trim_image(im):\n",
    "    \"\"\"Automatically crop the empty space around an image, preserving transparency\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    original_mode = im.mode\n",
    "    \n",
    "    # For transparent images, we need to check the alpha channel\n",
    "    if original_mode == 'RGBA':\n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(im)\n",
    "        # Get the alpha channel\n",
    "        alpha = img_array[:, :, 3]\n",
    "        # Find non-transparent pixels\n",
    "        mask = alpha > 0\n",
    "        # If image is entirely transparent, return it as is\n",
    "        if not np.any(mask):\n",
    "            return im\n",
    "        # Get bounding box of non-transparent pixels\n",
    "        rows = np.any(mask, axis=1)\n",
    "        cols = np.any(mask, axis=0)\n",
    "        y_min, y_max = np.where(rows)[0][[0, -1]]\n",
    "        x_min, x_max = np.where(cols)[0][[0, -1]]\n",
    "        # Crop without padding\n",
    "        return im.crop((x_min, y_min, x_max + 1, y_max + 1))\n",
    "    \n",
    "    # For non-transparent images\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    \n",
    "    if bbox:\n",
    "        # No padding\n",
    "        return im.crop(bbox)\n",
    "    return im\n",
    "\n",
    "# Load and process the CSV data\n",
    "df = pd.read_csv(KAGGLE_CSV)\n",
    "print(f\"\\nLoaded {len(df)} Pokemon from CSV\")\n",
    "\n",
    "# Clean the data and create a JSON structure\n",
    "pokemon_data = {}\n",
    "for _, row in df.iterrows():\n",
    "    pokemon_data[row[\"Name\"].lower()] = {\n",
    "        \"type1\": row[\"Type1\"],\n",
    "        \"type2\": row[\"Type2\"] if pd.notna(row[\"Type2\"]) else None,\n",
    "        \"evolution\": row[\"Evolution\"] if pd.notna(row[\"Evolution\"]) else None,\n",
    "    }\n",
    "\n",
    "# Save metadata as JSON\n",
    "with open(METADATA_PATH, \"w\") as f:\n",
    "    json.dump(pokemon_data, f, indent=2)\n",
    "print(f\"Saved metadata to: {METADATA_PATH}\")\n",
    "\n",
    "# Process images - crop and save to our data directory\n",
    "image_count = 0\n",
    "for img_file in os.listdir(KAGGLE_IMAGES):\n",
    "    if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "        src = os.path.join(KAGGLE_IMAGES, img_file)\n",
    "        dst = os.path.join(IMAGE_DIR, img_file.lower())\n",
    "        \n",
    "        # Open, crop, and save the image\n",
    "        try:\n",
    "            img = Image.open(src)\n",
    "            cropped_img = trim_image(img)\n",
    "            cropped_img.save(dst)\n",
    "            image_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {e}\")\n",
    "            # Fall back to copying the original if cropping fails\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"Processed {image_count} images to: {IMAGE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Pokemon data saved to 'updated_pokemon_metadata.json'\n",
      "\n",
      "Sample updated data for bulbasaur:\n",
      "{\n",
      "  \"type1\": \"Grass\",\n",
      "  \"type2\": \"Poison\",\n",
      "  \"evolution\": \"ivysaur\",\n",
      "  \"HP\": 45,\n",
      "  \"Attack\": 49,\n",
      "  \"Defense\": 49,\n",
      "  \"Sp_Attack\": 65,\n",
      "  \"Sp_Defense\": 65,\n",
      "  \"Speed\": 45,\n",
      "  \"height\": 0.7,\n",
      "  \"weight\": 6.9\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Load your existing Pokemon data (assuming it's saved as JSON already)\n",
    "with open(\"/app/data/metadata/images.json\", \"r\") as f:\n",
    "    existing_pokemon_data = json.load(f)\n",
    "\n",
    "# Fetch the new data with additional attributes from GitHub\n",
    "pokedex_url = (\n",
    "    \"https://raw.githubusercontent.com/Purukitto/pokemon-data.json/master/pokedex.json\"\n",
    ")\n",
    "response = requests.get(pokedex_url)\n",
    "if response.status_code == 200:\n",
    "    new_pokemon_data = response.json()\n",
    "else:\n",
    "    raise Exception(f\"Failed to fetch data: HTTP {response.status_code}\")\n",
    "\n",
    "# Create a mapping from lowercase English names to the new data\n",
    "new_pokemon_map = {}\n",
    "for pokemon in new_pokemon_data:\n",
    "    name_lower = pokemon[\"name\"][\"english\"].lower()\n",
    "    new_pokemon_map[name_lower] = pokemon\n",
    "\n",
    "\n",
    "# Function to convert height and weight strings to numeric values\n",
    "def extract_number(value_str):\n",
    "    if not value_str:\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+\\.?\\d*)\", value_str)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "# Update the existing data with the new attributes\n",
    "for name, data in existing_pokemon_data.items():\n",
    "    if name in new_pokemon_map:\n",
    "        new_data = new_pokemon_map[name]\n",
    "\n",
    "        # Add base stats\n",
    "        if \"base\" in new_data:\n",
    "            data[\"HP\"] = new_data[\"base\"][\"HP\"]\n",
    "            data[\"Attack\"] = new_data[\"base\"][\"Attack\"]\n",
    "            data[\"Defense\"] = new_data[\"base\"][\"Defense\"]\n",
    "            data[\"Sp_Attack\"] = new_data[\"base\"][\"Sp. Attack\"]\n",
    "            data[\"Sp_Defense\"] = new_data[\"base\"][\"Sp. Defense\"]\n",
    "            data[\"Speed\"] = new_data[\"base\"][\"Speed\"]\n",
    "\n",
    "        # Add height and weight as numeric values\n",
    "        if \"profile\" in new_data:\n",
    "            if \"height\" in new_data[\"profile\"]:\n",
    "                data[\"height\"] = extract_number(new_data[\"profile\"][\"height\"])\n",
    "\n",
    "            if \"weight\" in new_data[\"profile\"]:\n",
    "                data[\"weight\"] = extract_number(new_data[\"profile\"][\"weight\"])\n",
    "\n",
    "# Save the updated data\n",
    "with open(METADATA_PATH, \"w\") as f:\n",
    "    json.dump(existing_pokemon_data, f, indent=2)\n",
    "\n",
    "print(\"Updated Pokemon data saved to 'updated_pokemon_metadata.json'\")\n",
    "\n",
    "# Print a sample of the updated data structure\n",
    "sample_pokemon = list(existing_pokemon_data.keys())[0]\n",
    "print(f\"\\nSample updated data for {sample_pokemon}:\")\n",
    "print(json.dumps(existing_pokemon_data[sample_pokemon], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Validation ===\n",
      "\n",
      "Found 809 images\n",
      "Found 809 pokemon in metadata\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Dataset Validation ===\")\n",
    "\n",
    "# Check images\n",
    "image_files = [\n",
    "    f.lower()\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "]\n",
    "print(f\"\\nFound {len(image_files)} images\")\n",
    "\n",
    "# Check metadata\n",
    "with open(METADATA_PATH) as f:\n",
    "    metadata = json.load(f)\n",
    "print(f\"Found {len(metadata)} pokemon in metadata\")\n",
    "\n",
    "# Cross-reference\n",
    "images_without_metadata = []\n",
    "metadata_without_images = []\n",
    "\n",
    "# Check for images without metadata\n",
    "for img_file in image_files:\n",
    "    pokemon_name = os.path.splitext(img_file)[0].lower()\n",
    "    if pokemon_name not in metadata:\n",
    "        images_without_metadata.append(img_file)\n",
    "\n",
    "# Check for metadata without images\n",
    "for pokemon_name in metadata:\n",
    "    if not any(pokemon_name in img_file for img_file in image_files):\n",
    "        metadata_without_images.append(pokemon_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Image Statistics ===\n",
      "\n",
      "Found 716 different image sizes:\n",
      "- (99, 100): 1 images\n",
      "- (88, 100): 2 images\n",
      "- (55, 75): 1 images\n",
      "- (59, 73): 1 images\n",
      "- (119, 96): 1 images\n",
      "...\n",
      "\n",
      "Image formats: PNG\n",
      "\n",
      "=== Type Statistics ===\n",
      "\n",
      "Primary Types:\n",
      "- Water: 114\n",
      "- Normal: 105\n",
      "- Grass: 78\n",
      "- Bug: 72\n",
      "- Fire: 53\n",
      "- Psychic: 53\n",
      "- Rock: 46\n",
      "- Electric: 40\n",
      "- Poison: 34\n",
      "- Ground: 32\n",
      "- Dark: 29\n",
      "- Fighting: 29\n",
      "- Ghost: 27\n",
      "- Dragon: 27\n",
      "- Steel: 26\n",
      "- Ice: 23\n",
      "- Fairy: 18\n",
      "- Flying: 3\n",
      "\n",
      "Secondary Types:\n",
      "- Flying: 95\n",
      "- Poison: 32\n",
      "- Ground: 32\n",
      "- Fairy: 29\n",
      "- Psychic: 29\n",
      "- Fighting: 25\n",
      "- Steel: 23\n",
      "- Grass: 19\n",
      "- Dragon: 18\n",
      "- Water: 17\n",
      "- Dark: 17\n",
      "- Ghost: 16\n",
      "- Rock: 14\n",
      "- Fire: 11\n",
      "- Ice: 11\n",
      "- Electric: 8\n",
      "- Bug: 5\n",
      "- Normal: 4\n"
     ]
    }
   ],
   "source": [
    "# Image statistics\n",
    "print(\"\\n=== Image Statistics ===\")\n",
    "sizes = []\n",
    "formats = set()\n",
    "corrupt_images = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(IMAGE_DIR, img_file)\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            sizes.append(img.size)\n",
    "            formats.add(img.format)\n",
    "    except Exception as e:\n",
    "        corrupt_images.append((img_file, str(e)))\n",
    "\n",
    "unique_sizes = set(sizes)\n",
    "print(f\"\\nFound {len(unique_sizes)} different image sizes:\")\n",
    "for size in list(unique_sizes)[:5]:\n",
    "    count = sizes.count(size)\n",
    "    print(f\"- {size}: {count} images\")\n",
    "if len(unique_sizes) > 5:\n",
    "    print(\"...\")\n",
    "\n",
    "print(f\"\\nImage formats: {', '.join(formats)}\")\n",
    "\n",
    "if corrupt_images:\n",
    "    print(\"\\n=== Corrupt Images ===\")\n",
    "    print(f\"Found {len(corrupt_images)} corrupt images:\")\n",
    "    for img, error in corrupt_images[:5]:\n",
    "        print(f\"- {img}: {error}\")\n",
    "    if len(corrupt_images) > 5:\n",
    "        print(\"...\")\n",
    "\n",
    "# Type statistics\n",
    "print(\"\\n=== Type Statistics ===\")\n",
    "primary_types = df[\"Type1\"].value_counts()\n",
    "secondary_types = df[\"Type2\"].value_counts()\n",
    "\n",
    "print(\"\\nPrimary Types:\")\n",
    "for type_name, count in primary_types.items():\n",
    "    print(f\"- {type_name}: {count}\")\n",
    "\n",
    "print(\"\\nSecondary Types:\")\n",
    "for type_name, count in secondary_types.items():\n",
    "    if pd.notna(type_name):\n",
    "        print(f\"- {type_name}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
